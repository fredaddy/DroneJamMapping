{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "681d0c18-b738-4722-9a19-3a34734c14f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Apr 24 2024 11:19:34\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "class DroneJammingEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(DroneJammingEnv, self).__init__()\n",
    "\n",
    "        # Connection to PyBullet\n",
    "        try:\n",
    "            p.disconnect()\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        self.client = p.connect(p.GUI)\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "\n",
    "        # Define action and observation spaces\n",
    "        # [dx, dy, dz]\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        # [x, y, z, signal]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32)\n",
    "\n",
    "        # Load drone model\n",
    "        self.drone = p.loadURDF(\"./quadrotor.urdf\", [0, 0, 1])\n",
    "\n",
    "        # Set simulation parameters\n",
    "        self.current_step = 0\n",
    "        self.time_step = 1.0\n",
    "        self.max_steps = 1000\n",
    "        self.signal_strength = 0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply force\n",
    "        done = False\n",
    "        thrust_x, thrust_y, thrust_z = self._map_discrete_action(action)\n",
    "        p.applyExternalForce(self.drone, -1, forceObj=np.array([thrust_x, thrust_y, thrust_z]) * 100, posObj=[0, 0, 0], flags=p.LINK_FRAME)\n",
    "        p.stepSimulation()\n",
    "\n",
    "        pos, ori = p.getBasePositionAndOrientation(self.drone)\n",
    "        self.drone_position = pos\n",
    "\n",
    "        self.adjust_camera_angle()\n",
    "\n",
    "\n",
    "        # Signal reward\n",
    "        distance = np.linalg.norm(pos - np.array([5, 5, 1]))\n",
    "        signal = 1e5/(distance ** 2) \n",
    "    \n",
    "        reward = signal\n",
    "        self.state =  np.concatenate([np.array(pos), np.array([signal])])\n",
    "        \n",
    "        self.current_step += 1\n",
    "        \n",
    "        if self.current_step > self.max_steps: \n",
    "            done = True\n",
    "\n",
    "        if pos[-1] <= 0.1 or pos[-1] >= 5:\n",
    "            reward -= 1e5\n",
    "            done = True\n",
    "                        \n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def _map_discrete_action(self, action):\n",
    "        discrete_thrust_levels = [\n",
    "            [1, 0, 0],\n",
    "            [-1, 0, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, -1, 0],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, -1]\n",
    "        ]\n",
    "        return discrete_thrust_levels[action]\n",
    "\n",
    "    def adjust_camera_angle(self):\n",
    "        # Example: Move the camera closer to the scene\n",
    "        camera_position = [self.drone_position[0], self.drone_position[1], 5]\n",
    "        target_position = self.drone_position\n",
    "        up_vector = [0, 0, 1] \n",
    "    \n",
    "        # Compute view matrix directly from camera parameters\n",
    "        view_matrix = p.computeViewMatrix(cameraEyePosition=camera_position,\n",
    "                                           cameraTargetPosition=target_position,\n",
    "                                           cameraUpVector=up_vector)\n",
    "    \n",
    "        # Set the new view matrix for the camera\n",
    "        p.resetDebugVisualizerCamera(cameraDistance=5, cameraYaw=0, cameraPitch=-30,\n",
    "                                      cameraTargetPosition=target_position)\n",
    "\n",
    "    def reset(self):\n",
    "        p.resetSimulation()\n",
    "        p.setGravity(0, 0, -9.8)\n",
    "        p.loadURDF(\"plane.urdf\", [0, 0, 0])\n",
    "        self.drone = p.loadURDF(\"quadrotor.urdf\", [0, 0, 1])\n",
    "        self.drone_position = np.array([0, 0, 1])\n",
    "        \n",
    "        # Set initial state\n",
    "        self.state = np.zeros(4)\n",
    "        self.current_step = 0 \n",
    "        \n",
    "        return self.state\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        p.getCameraImage(width=640, height=480)\n",
    "        \n",
    "    def close(self):\n",
    "        p.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c936d1b-6f28-4dee-b7c8-e83e5eadd085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelchian/opt/anaconda3/envs/cs234proj/lib/python3.9/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f90eda9660149449a23c48d6e9d4bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "| time/              |    |\n",
      "|    fps             | 19 |\n",
      "|    iterations      | 1  |\n",
      "|    time_elapsed    | 3  |\n",
      "|    total_timesteps | 64 |\n",
      "---------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 66            |\n",
      "|    ep_rew_mean          | 4.58e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 20            |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 128           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0489097e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.79         |\n",
      "|    explained_variance   | -1.43e-06     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.47e+08      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -2e-05        |\n",
      "|    value_loss           | 8.93e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 66            |\n",
      "|    ep_rew_mean          | 4.58e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 23            |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 8             |\n",
      "|    total_timesteps      | 192           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3504177e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.79         |\n",
      "|    explained_variance   | -1.31e-06     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.25e+08      |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -5.08e-05     |\n",
      "|    value_loss           | 8.51e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 99            |\n",
      "|    ep_rew_mean          | 8.53e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 22            |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 11            |\n",
      "|    total_timesteps      | 256           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.6251884e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.79         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.18e+08      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.000102     |\n",
      "|    value_loss           | 4.36e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 99            |\n",
      "|    ep_rew_mean          | 8.53e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 23            |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 13            |\n",
      "|    total_timesteps      | 320           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.0152025e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.79         |\n",
      "|    explained_variance   | -1.55e-06     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.91e+08      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.000142     |\n",
      "|    value_loss           | 1.18e+09      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 109           |\n",
      "|    ep_rew_mean          | 1.01e+05      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 22            |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 17            |\n",
      "|    total_timesteps      | 384           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.7183206e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.79         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.57e+08      |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | -6.17e-05     |\n",
      "|    value_loss           | 5.13e+08      |\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = DroneJammingEnv()\n",
    "\n",
    "# Initialize the PPO agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, n_steps=64)\n",
    "\n",
    "# Train the model\n",
    "total_timesteps = 1e3\n",
    "model.learn(total_timesteps=total_timesteps, progress_bar=True)\n",
    "\n",
    "print(\"im done\")\n",
    "# Save the model\n",
    "model.save(\"ppo_drone\")\n",
    "\n",
    "# Load the model\n",
    "model = PPO.load(\"ppo_drone\")\n",
    "\n",
    "# Evaluate the model\n",
    "num_episodes = 10\n",
    "for episode in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    ep_length = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        print(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        ep_reward += reward\n",
    "        ep_length += 1\n",
    "        env.render()\n",
    "    \n",
    "    print(f\"Episode {episode + 1}: Reward = {ep_reward}, Length = {ep_length}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97140192-8ee7-40ab-88d7-b3527cc5347d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd4a1c-1c9f-481f-86c9-c7ce479c8b96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
